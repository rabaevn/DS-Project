---
title: "Data Cleaning Group 10"
author: "Neomi Rabaev, Yael Snear, Yarin Swisa, Liat Vaizman"
date: "2024-06-29"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# load necessary libraries
library(tidyverse)
library(dplyr)
```

## Data Cleaning

```{r eval=TRUE}
#load the data
data <- read_tsv("data/equal_in_quality_may22.tsv", col_types = cols(.default = col_guess()))
parsing_issues <- problems(data)
parsing_issues
glimpse(data)
view(data)
```
```{r}
# remove rows with parsing issues - it means the link is not good for us
# either the link is not available or in a different language than neede
if (nrow(parsing_issues) > 0) {
  problematic_rows <- parsing_issues$row
  data <- data[-problematic_rows, ]
}
```


```{r}
#remove all rows where link is not available or in a different language (regardless of the parsing issues)
# link accessibility = 2 -> link not available
# link accessibility = 3 -> link in a different language
data <- data %>%
  filter(link_accessibility != 2 & link_accessibility != 3) # maybe also delete 4?
```


```{r}
toDrop <- c("site_type", "gl", "hl", "term", "code_lang", "collection_date", "link", "domain", "top_level_domain", "coding_completion_date", "does_the_content_reject_or_reinforce_the_disc", "is_malicious_meaning_is_alluded_to_in_the_tex", "do_claims_contradict_accepted_scientific_expl", "are_specific_groups_in_society_pharma_jews_mu", "is_there_reference_to_everyday_life", "langs_country", "lang_short", "lang_uniq", "content_producer_fct", "site_type_collapsed", "if_you_answered_no_to_the_previous_question_y", "what_is_the_author_background", "are_there_major_scientific_errors_in_the_link", "how_accurate_is_the_scientific_content_presen", "are_there_any_scientific_terms_that_lay_peopl", "is_a_scientific_background_high_school_or_abo", "are_the_following_sources_available_in_the_li", "x1_none_of_the_below", "x2_others", "x3_place_to_leave_comments_ask_questions", "x4_graphs", "x5_relevant_multimedia_e_g_video_animation_et", "x6_hyper_links", "x7_numerical_data", "x8_list_of_sources", "x9_citation_or_references_to_relevant_literat", "producer_high_cred", "cons_n_neg", "cons_n_pos", "recency", "conspiracy", "agg_sci_qual")

# remove unnecessary columns and rows with NA's in the most important column
cleaned_data <- data %>%
  #filter(conspiracy != 1) %>% # check with nir 
  filter(!is.na(agg_sci_qual)) %>%
  filter(type != "Conspiracy Theories") %>%
  mutate(country = str_extract(country, "(?<=,)[^,]+$")) %>%# leave only the name of the country
  mutate(what_is_the_author_background = ifelse(is.na(what_is_the_author_background), 1, what_is_the_author_background))%>%
  # Calculate the most frequent value in the column
  mutate(site_type_fct = coalesce(site_type_fct, 
                                   names(sort(table(site_type_fct), decreasing = TRUE))[1])) %>%
  # replace na's in jargon with 0
  mutate(jargon_score = replace_na(jargon_score, 0)) %>%
  mutate(are_there_major_scientific_errors_in_the_link = replace_na(are_there_major_scientific_errors_in_the_link, 0)) %>%
  select(-all_of(toDrop)) 
glimpse(cleaned_data)
```

```{r}
# replace NA's with the most common for each language
# Calculate the most common content_producer for each language
most_common_producer <- cleaned_data %>%
  filter(!is.null(content_producer)) %>%
  group_by(langs, content_producer) %>%
  summarise(count = n()) %>%
  arrange(desc(count)) %>%
  slice(1) %>%
  select(langs, content_producer)

# Join the most common content_producer back to the cleaned_data
cleaned_data <- cleaned_data %>%
  left_join(most_common_producer, by = "langs", suffix = c("", "_most_common")) %>%
  mutate(content_producer = ifelse(is.null(content_producer), content_producer_most_common, content_producer)) %>%
  select(-content_producer_most_common)
```
```{r}
# Replace NULL values in the how_recent_is_the_information column
cleaned_data <- cleaned_data %>%
  group_by(langs) %>%
  mutate(how_recent_is_the_information = ifelse(
    is.na(how_recent_is_the_information),
    # Find the most common value (mode) for each language
    names(sort(table(how_recent_is_the_information), decreasing = TRUE))[1],
    how_recent_is_the_information
  )) %>%
  ungroup()

```




```{r}
library(dplyr)

# Function to replace NULL values with the most common value per language
replace_null_with_mode <- function(data, target_col, group_col) {
  data %>%
    group_by(!!sym(group_col)) %>%
    mutate(!!sym(target_col) := ifelse(is.na(!!sym(target_col)),
                                       as.numeric(names(sort(table(!!sym(target_col)), decreasing = TRUE)[1])),
                                       !!sym(target_col))) %>%
    ungroup()
}

# Replace NULL values in the specified columns
cleaned_data <- replace_null_with_mode(cleaned_data, "local_examples_prop", "langs")
cleaned_data <- replace_null_with_mode(cleaned_data, "does_the_content_present_advantages_and_disad", "langs")
cleaned_data <- replace_null_with_mode(cleaned_data, "are_there_local_examples_in_the_content_e_g_r", "langs")
cleaned_data <- replace_null_with_mode(cleaned_data, "access", "langs")
```



```{r}
# convert double columns to factors (categorial column)
columns_to_convert <- c("content_producer", "site_type_fct", "type", "link_type", "link_accessibility", "jargon_score", "are_there_local_examples_in_the_content_e_g_r", "does_the_content_present_advantages_and_disad")

cleaned_data <- cleaned_data %>%
  mutate(across(all_of(columns_to_convert), as.factor))
```




```{r}
# convert columns to ordinal values
# specify their levels
ordinal_columns <- list(
  access_multimedia = 0:4,
  access_sci = 0:2,
  accuracy = 0:2,
  cred = 0:4,
  contextual = 0:3,
  access = 0:4
)

cleaned_data <- cleaned_data %>%
  mutate(across(all_of(names(ordinal_columns)), ~ factor(., levels = ordinal_columns[[as.character(cur_column())]])))
```


```{r}

# Calculate percentage of missing values per column
missing_summary <- colSums(is.na(cleaned_data)) / nrow(cleaned_data) * 100
missing_summary_df <- data.frame(variable = names(missing_summary), percent_missing = missing_summary)
missing_summary_df <- missing_summary_df[order(-missing_summary_df$percent_missing), ]


# Create a list of variables with their percentage of missing values
missing_list <- paste(missing_summary_df$variable, ": ", round(missing_summary_df$percent_missing, 2), "%", sep = "")

# Print the list
print(missing_list)

```
```{r}
# get the clean data to a file to work on
write.table(cleaned_data, file = "cleaned_data.tsv", sep = "\t", row.names = FALSE, quote = FALSE)
```

GREAT SUCCESS